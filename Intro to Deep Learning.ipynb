{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781931c7",
   "metadata": {},
   "source": [
    "# INTRO TO DEEP LEARNING (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe0708",
   "metadata": {},
   "source": [
    "### LECTURE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63631848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #importing tensorflow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366257ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#few Non linear activation functions: \n",
    "\"\"\"\n",
    "tf.math.sigmoid(z)\n",
    "tf.math.tanh(z)\n",
    "tf.nn.relu(z)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense Layer:\n",
    "\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        \n",
    "        self.w=self.add_weight([input_dim,output_dim])\n",
    "        self.b=self.add_weight([1,output_dim])\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        z=tf.matmul(inputs, self.w)+self.b\n",
    "        output=tf.math.sigmoid(z)\n",
    "        \n",
    "        return output\n",
    "                #OR\n",
    "        \n",
    "layer= tf.keras.layers.Dense(units=2) #units--> no. of output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0cccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential Model:\n",
    "\n",
    "#For one hidden layer:\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n),       #n hidden units in the hidden layer\n",
    "    tf.keras.layers.Dense(2)        #2 output units \n",
    "])\n",
    "\n",
    "#For multiple hidden layers, we stack each one of them>>\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n1),\n",
    "    tf.keras.layers.Dense(n2),\n",
    "    tf.keras.layers.Dense(n3),\n",
    "    tf.keras.layers.Dense(2) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss functions:\n",
    "\n",
    "loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,pred)) #binary cross entropy for binary classification\n",
    "\n",
    "loss=tf.reduce_mean(tf.square(tf.subtract(y,pred)))\n",
    "loss=tf.keras.losses.MSE(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9684ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the NN, using Gradient Descent:\n",
    "\n",
    "model = tf.keras.Sequential([...])\n",
    "\n",
    "optimizer=tf.keras.optimizers.SGD() #can pick any optimizer in tensorflow\n",
    "while True:\n",
    "    pred = model(x) #forward passing thru the network\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss= compute_loss(y,pred) #computing the loss\n",
    "    grads=tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c33ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularization\n",
    "#1:\n",
    "tf.keras.layers.Dropout(p=0.5) #drops 50% of activations in a layer\n",
    "#2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa5005",
   "metadata": {},
   "source": [
    "### LECTURE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN intuition:\n",
    "\n",
    "myrnn=RNN()\n",
    "hidden_state=[0,0,0,0]\n",
    "sentence= [\"I\",\"love\",\"recurrent\",\"neural\"]\n",
    "\n",
    "for word in sentence:\n",
    "    pred, hidden_state=myrnn(word,hidden_state)\n",
    "next_word_pred=pred\n",
    "#<<networks>> is given as output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing RNN using tf\n",
    "tf.keras.layers.SimpleRNN(rnn_units) #rnn_units--> no. of recurrent cells if im not wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4080fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing LSTM using tf\n",
    "tf.keras.layers.LSTM(num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538dd75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
